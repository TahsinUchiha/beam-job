{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation on Apache Beam Job\n",
    "\n",
    "This documentation servers as a purpose of the steps/procedures taken during the development of `beam_pipeline.py` and `beam_pipeline_composite.py`, the entire process can also be executed from `runbeam.sh` script. Tests can be executed by running `runtests.sh`. Documentation here servers as a purpose of reproducing the steps and any learning occurred during the process. \n",
    "\n",
    "\n",
    "## Steps \n",
    "There are three ways: \n",
    "- First we can pull the file locally and process it locally. This however may still require authentication when using a gsutil command. \n",
    "To pull the file this way, since file exists in \n",
    "`gs://cloud-samples-data/bigquery/sample-transactions/transactions.csv`\n",
    "We will still need to configure `gcloud`, and use `gsutil` to copy it. The disadvantages here is that file could be huge and would take up significant storage space.\n",
    "- Second, we can directly give the path in our pipeline and read from there. This has advantage if the file is exceptionally large, allowing us to directly process from cloud.\n",
    "- Third, and what we will be doing in our case will be using the `requests` library to do a get request, download the file and process it directly. This has the most advantage as we can avoid authentication steps this way and directly process the file. \n",
    "\n",
    "### **To configure for first two suggestions (Otherwise skip to [Step 2](#Step-2))**\n",
    "\n",
    "We will still need to configure `gcloud`, and use `gsutil` to copy it.\n",
    "- Directly give the path in our pipeline and read from there. This has advantage if the file is exceptionally large, allowing us to directly process. \n",
    "\n",
    "### Step 1.1\n",
    "- Configure `gcloud`\n",
    "\t- Install gcloud SDK - https://cloud.google.com/sdk/docs/install-sdk\n",
    "\t- Run `gcloud init` - initialise your gcloud account, authenticate, select project.\n",
    "- Next copy file using `gsutil` to your desired directory, in my case in the current directory. \n",
    "    ```gsutil cp gs://cloud-samples-data/bigquery/sample-transactions/transactions.csv .```\n",
    "\n",
    "### Step 1.2 \n",
    "With this step we can configure our beam to Read directly from `gs://cloud-samples-data/bigquery/sample-transactions/transactions.csv`\n",
    "- `pip install apache-beam[gcp]`\n",
    "- Run the following steps before executing:\n",
    "\t- `gcloud auth application-default login`\n",
    "\t- `export GOOGLE_APPLICATION_CREDENTIALS=<credential_file>.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start writing the pipeline.\n",
    "### Step 2\n",
    "If you haven't done so already, you will need to install apache-beam, \n",
    "- `pip install apache-beam[gcp]` \n",
    "- Create beam_pipeline.py\n",
    "\t- Import the necessary packages which will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import csv\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the file ready, specifying the `https://https://storage.googleapis.com/` link, as this is where the location of the file will be. \n",
    "\n",
    "- Create the `run()` function which will be called and the first Beam pipeline, `transactions` which will have:\n",
    "    - First line in the pipeline, 'Read CSV' reads from input, if we wanted to read directly from `gs://` this would require additional authentication. However since we are downloading the file via get request, we can simply read the CSV file that we have processed. \n",
    "        - The `beam.Create(lines)` creates a PCollection from the lines list, where each element in the PCollection is a line. \n",
    "    - Next line uses the Map transform to apply a function to each element in the **PCollection** (the output of the previous transform), we use a lambda function that uses csv.reader to parse each line as CSV. \n",
    "    - For the next 3 PTransforms, I am first converting each row into a tuple so that we can apply the transform `beam.Distinct()`, since this requires input to be hashable and lists are not hashable in python. The `beam.Distinct()` is applied to remove duplicate rows from the PCollection. Once applied, we turn it back into a list. \n",
    "    - Next we filter the header row, so only the data is processed. In this case we know the first column is `timestamp`\n",
    "    \n",
    "- Next part of the pipeline transformation chain is to create our filter conditions, `filtered_transactions`, by pulling from the previous transactions pipeline\n",
    "    - First part filters for the column = transaction_amount, values > 20\n",
    "    - Second part filters for the timestamp >= 2010 \n",
    "\n",
    "- Then to calculate total, we create the `totals` pipeline, starting from previous pipeline\n",
    "    - The first part creates a tuple of (date, transaction_amount), stripping the timestamp into the \n",
    "    `YYYY-mm-dd` format.\n",
    "    - Second part, `beam.CombinePerKey(sum)` combines the float values for each date using the `sum` function.\n",
    "\n",
    "- Finally we write to a file `output/results.jsonl.gz`, giving the header, compression_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS File\n",
    "file='https://storage.googleapis.com/cloud-samples-data/bigquery/sample-transactions/transactions.csv' \n",
    "\n",
    "# Download the file from GCS\n",
    "response = requests.get(file)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print('Successfully retrieved file')\n",
    "    lines = response.text.split('\\n')\n",
    "else:\n",
    "    print(f\"Failed to download file: {response.status_code}, {response.text}\")\n",
    "    lines = []\n",
    "\n",
    "def run(argv=None):\n",
    "    # Set up the pipeline options\n",
    "    options = PipelineOptions()\n",
    "    with beam.Pipeline(options = options) as pipeline:\n",
    "        # Read the input CSV file\n",
    "        transactions = (\n",
    "            pipeline\n",
    "            # | 'Read CSV' >> beam.io.ReadFromText('gs://cloud-samples-data/bigquery/sample-transactions/transactions.csv')\n",
    "            | 'Read CSV' >> beam.Create(lines)\n",
    "            | beam.Map(lambda line: next(csv.reader([line])))\n",
    "            | beam.Map(lambda row: tuple(row))  # Convert list to tuple \n",
    "            | 'Remove duplicates' >> beam.Distinct()  # Remove duplicates based on the key\n",
    "            | beam.Map(lambda row: list(row))  # Convert the tuple back to a list\n",
    "            | beam.Filter(lambda row: row[0] != 'timestamp')\n",
    "        )\n",
    "        \n",
    "        # Filter for amounts > 20 and year >= 2010\n",
    "        filtered_transactions = (\n",
    "            transactions\n",
    "            | beam.Filter(lambda row: float(row[3]) > 20) # check transaction amount greater than 20\n",
    "            | beam.Filter(lambda row: int(row[0].split('-')[0]) >= 2010) # only for year >= 2010\n",
    "        )\n",
    "        # Calculate the total transaction amount grouped by date \n",
    "        \n",
    "        totals = (\n",
    "            filtered_transactions\n",
    "            | beam.Map(lambda row: (datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S %Z').strftime('%Y-%m-%d'), float(row[3])))\n",
    "            | beam.CombinePerKey(sum)\n",
    "        )\n",
    "        \n",
    "        # Write output as a JSONL file compressed\n",
    "        totals | 'Write JSONL' >> beam.io.WriteToText('output/results.jsonl.gz', file_name_suffix='.gz', header='date,total-amount', compression_type=beam.io.textio.CompressionTypes.GZIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly run the pipeline\n",
    "- Run from command line\n",
    "    `python3 beam_pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "For this we combine the transformation procedures into a single composite transform class, this is done so that the processing logic is reusable, making code cleaner and easier to understand. \n",
    "Because Composite transforms can be nested, it allows us to build a complex pipeline in a modular way, making code easier to maintain and extend. \n",
    "\n",
    "So we start by creating a class `TransactionProcessing`, which will be made up of several `Map`, `Filter` and `CombinePerKey` transforms as used previously in the `beam_pipeline.py`. For the sake understanding, I have created an additional pipeline `beam_pipeline_composite.py`, which simply calls `TransactionProcessing()` pipeline between Read/Write, thus seperating the bulk of our processing in the `transaction_processing.py`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "\n",
    "# The below class extends beam.PTransform, which is the base class. \n",
    "# This TransactionProcessing class is for the 'transactions.csv' file, \n",
    "# It filters for transaction_amount > 20 and year(timestamp) >= 2010 \n",
    "# Lastly sums the result, outputting (date,transaction-amount)\n",
    "\n",
    "class TransactionProcessing(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | beam.Map(lambda line: next(csv.reader([line])))\n",
    "            | beam.Map(lambda row: tuple(row))  # Convert list to tuple \n",
    "            | 'Deduplicate elements' >> beam.Distinct()  # Remove duplicates \n",
    "            | beam.Map(lambda row: list(row))  # Convert the tuple back to a list\n",
    "            | beam.Filter(lambda row: row[0] != 'timestamp')            \n",
    "            | beam.Filter(lambda row: float(row[3]) > 20)\n",
    "            | beam.Filter(lambda row: int(row[0].split('-')[0]) >= 2010)\n",
    "            | beam.Map(lambda row: (datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S %Z').strftime('%Y-%m-%d'), float(row[3])))\n",
    "            | beam.CombinePerKey(sum)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expand name is used because it is an extension of the PTransform, as part of Beam's conventions and ensure transform works correctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Testing\n",
    "\n",
    "Import the necessary modules, in this case we will be using `assert_that` and `equal_to` to create our unit test cases. \n",
    "Additional modules also imported, including the composite transform class created. \n",
    "We create a testclass as `TransactionProcessingTest`, this will have multiple unit tests. \n",
    "- First we will test on a single transaction.\n",
    "- Second we will test on filtered conditions.\n",
    "- Third we will test if it sums on the same date.\n",
    "- Lastly we will test on the sample transaction.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import apache_beam as beam\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "import unittest.mock\n",
    "from apache_beam.testing.test_pipeline import TestPipeline\n",
    "from apache_beam.testing.util import assert_that\n",
    "from apache_beam.testing.util import equal_to\n",
    "from transforms.transaction_processing import TransactionProcessing\n",
    "\n",
    "class TransactionProcessingTest(unittest.TestCase):\n",
    "    # Expected output\n",
    "    def test_single_transaction(self):\n",
    "        with TestPipeline(runner='DirectRunner') as p:\n",
    "            input_data = [\n",
    "                '2017-03-18 14:09:16 UTC,wallet00001866cb7e0f09a890,wallet00001e494c12b3083634,2102.22'\n",
    "            ]\n",
    "            expected_output = [\n",
    "                ('2017-03-18', 2102.22)\n",
    "            ]\n",
    "            output = (p \n",
    "                        | beam.Create(input_data) \n",
    "                        | TransactionProcessing())\n",
    "            assert_that(output, equal_to(expected_output))\n",
    "            \n",
    "    # Show filtering for amount > 20 and year < 2010\n",
    "    def test_filtered_transactions(self):\n",
    "        with TestPipeline(runner='DirectRunner') as p:\n",
    "            input_data = [\n",
    "                '2009-01-09 02:54:25 UTC,wallet00000e719adfeaa64b5a,wallet00001866cb7e0f09a890,1021101.99',  # Before 2010\n",
    "                '2017-01-01 04:22:23 UTC,wallet00000e719adfeaa64b5a,wallet00001e494c12b3083634,19.95'  # Amount <= 20\n",
    "            ]\n",
    "            expected_output = []\n",
    "            result = p | beam.Create(input_data) | TransactionProcessing()\n",
    "            assert_that(result, equal_to(expected_output))\n",
    "\n",
    "    # Show summations \n",
    "    def test_same_date_transactions(self):\n",
    "        with TestPipeline(runner='DirectRunner') as p:\n",
    "            input_data = [\n",
    "                '2017-03-18 14:09:16 UTC,wallet00001866cb7e0f09a890,wallet00001e494c12b3083634,2102.22',\n",
    "                '2017-03-18 14:10:44 UTC,wallet00001866cb7e0f09a890,wallet00000e719adfeaa64b5a,21.0'\n",
    "            ]\n",
    "            expected_output = [\n",
    "                ('2017-03-18', 2123.22)\n",
    "            ]\n",
    "            result = p | beam.Create(input_data) | TransactionProcessing()\n",
    "            assert_that(result, equal_to(expected_output))\n",
    "            \n",
    "    # Entire transaction.csv process\n",
    "    def test_process(self):\n",
    "        with TestPipeline(runner='DirectRunner') as p:\n",
    "            input_data = [\n",
    "                '2009-01-09 02:54:25 UTC,wallet00000e719adfeaa64b5a,wallet00001866cb7e0f09a890,1021101.99',\n",
    "                '2017-01-01 04:22:23 UTC,wallet00000e719adfeaa64b5a,wallet00001e494c12b3083634,19.95',\n",
    "                '2017-03-18 14:09:16 UTC,wallet00001866cb7e0f09a890,wallet00001e494c12b3083634,2102.22',\n",
    "                '2017-03-18 14:10:44 UTC,wallet00001866cb7e0f09a890,wallet00000e719adfeaa64b5a,1.00030',\n",
    "                '2017-08-31 17:00:09 UTC,wallet00001e494c12b3083634,wallet00005f83196ec58e4ffe,13700000023.08',\n",
    "                '2018-02-27 16:04:11 UTC,wallet00005f83196ec58e4ffe,wallet00001866cb7e0f09a890,129.12'\n",
    "            ]\n",
    "            expected_output = [\n",
    "                ('2017-03-18', 2102.22),\n",
    "                ('2017-08-31', 13700000023.08),\n",
    "                ('2018-02-27', 129.12)\n",
    "            ]\n",
    "            result = p | beam.Create(input_data) | TransactionProcessing()\n",
    "            assert_that(result, equal_to(expected_output))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tests using `python3 -m unittest transaction_processing_test.py` \n",
    "This will check whether the TransactionProcessing() class is processing the data as expected. \n",
    "The lines  `assert_that(output, equal_to(expected_output))` checks whether the output of the pipeline is equal to the expected output, otherwise the test will fail. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
